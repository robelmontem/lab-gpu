{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.12 ms ± 6.48 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.4 ms ± 19.6 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.4 ms ± 28.1 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "%timeit -n 5 -r 2 grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "%timeit -n 5 -r 2 grade2_ufunc(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "%timeit -n 5 -r 2 a*a_cpu**2 + b*b_cpu + c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c552d6-5d68-4deb-9901-f5f351cc5769",
   "metadata": {},
   "source": [
    "# Inicialización\n",
    "En el bloque de código original no se especifica el tipo de dato usado, por lo que asumo que durante el resto de la práctica, a no ser que se pida, se usará el tipo por defecto para punto flotante (float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d57d55-1ddb-410e-af81-427c76f11ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "Using CuPy with : NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "#Inicializacion\n",
    "cuda_device = 0\n",
    "devices = cp.cuda.runtime.getDeviceCount()\n",
    "assert devices > 0, 'No CUDA-powered device found'\n",
    "print('Found {} CUDA devices'.format(devices))\n",
    "cp.cuda.runtime.setDevice(cuda_device)\n",
    "specs = cp.cuda.runtime.getDeviceProperties(cuda_device)\n",
    "name = specs['name'].decode()\n",
    "print('Using CuPy with : {}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f442ea8-29b0-4645-b583-db7d45ad954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce GTX 1080'                              [SUPPORTED]\n",
      "                      Compute Capability: 6.1\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-574ca04c-bd1f-2193-bb43-192268bb8508\n",
      "                                Watchdog: Disabled\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import vectorize, float64, cuda\n",
    "\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0a120-c736-4a8e-b9d5-6105816f2776",
   "metadata": {},
   "source": [
    "# Apartado 3.2\n",
    "## Ejercicio A\n",
    "\n",
    "Vamos a usar los arrays de *numpy* creados en el primer bloque de código, copiandolos a GPU con `CuPy.asarray` para poder calcular $z$ en GPU. Todo esto debe de hacerse dentro de la misma función para que se mida correctamente por la función `benchmark`. No se tendrá en cuenta el tiempo de creación de los arrays de *numpy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be648529-685b-439d-8e1a-4467b3797f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time grade2_cupy_copy: 17.208 ms\n"
     ]
    }
   ],
   "source": [
    "def grade2_cupy_copy(x, y, a, b, c):\n",
    "    # los arrays x e y ya están creados en CPU\n",
    "    x_gpu = cp.asarray(x)\n",
    "    y_gpu = cp.asarray(y)\n",
    "    return a*x_gpu**2 + b*y_gpu + c\n",
    "\n",
    "c_gpu = grade2_cupy_copy(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "assert c_cpu.all() == c_gpu.all(), 'CPU y GPU tienen resultados diferentes'\n",
    "\n",
    "execution_gpu = benchmark(grade2_cupy_copy, (a_cpu, b_cpu, a, b, c,), n_repeat=10, n_warmup=2)\n",
    "gpu_avg_time = np.average(execution_gpu.gpu_times) * 1e3  # convert to ms\n",
    "print(f\"Elapsed time grade2_cupy_copy: {gpu_avg_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dff1497-b755-4fe7-8016-701a50c38540",
   "metadata": {},
   "source": [
    "Ahora, se crearán en la función los arrays dentro de GPU con la función `CuPy.random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef039f4-1844-45a0-8c08-5cffc17e0508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time grade2_cupy_no_copy: 4.839 ms\n"
     ]
    }
   ],
   "source": [
    "def grade2_cupy_no_copy(x_gpu, y_gpu, a, b, c):\n",
    "    return a*x_gpu**2 + b*y_gpu + c\n",
    "\n",
    "# El resultado será distinto al de las anteriores ejecuciones\n",
    "x_gpu = cp.random.rand(size)\n",
    "y_gpu = cp.random.rand(size)\n",
    "execution_gpu = benchmark(grade2_cupy_no_copy, (x_gpu, y_gpu, a, b, c,), n_repeat=10, n_warmup=2)\n",
    "gpu_avg_time = np.average(execution_gpu.gpu_times) * 1e3  # convert to ms\n",
    "print(f\"Elapsed time grade2_cupy_no_copy: {gpu_avg_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33684a9d-0971-4536-8603-57da92f64148",
   "metadata": {},
   "source": [
    "Como se puede apreciar, hay un *overhead* considerable por copiar las variables a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f265a9-d64c-414c-918d-ff6dcd8d285c",
   "metadata": {},
   "source": [
    "## Ejercicio B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100f611-ff1e-4195-9a49-aebfed85dec3",
   "metadata": {},
   "source": [
    "Creamos la función `grade2_numba` que será llamada por las dos funciones pedidas (con y sin copia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315c3118-1d45-42f9-8ec3-96d5ef453f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize(['float64(float64, float64, float64, float64, float64)'], target = 'cuda')\n",
    "def grade2_numba(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# Función con copia\n",
    "def grade2_numba_copy(x, y, a, b, c):\n",
    "    return grade2_numba(x, y, a, b, c)\n",
    "    \n",
    "# Función sin copia\n",
    "def grade2_numba_no_copy(x, y, a, b, c):\n",
    "    c_gpu_numba = grade2_numba(x, y, a, b, c)\n",
    "    cuda.synchronize() \n",
    "    return c_gpu_numba\n",
    "\n",
    "# Copia manual de arrays a GPU\n",
    "a_gpu = cuda.to_device(a_cpu)\n",
    "b_gpu = cuda.to_device(b_cpu)\n",
    "\n",
    "# Tiempos con benchmark\n",
    "execution_gpu = benchmark(grade2_numba_copy, (a_cpu, b_cpu, a, b, c,), n_repeat=10, n_warmup=2)\n",
    "gpu_avg_time = np.average(execution_gpu.gpu_times) * 1e3  # convert to ms\n",
    "print(f\"Elapsed time grade2_numba_copy: {gpu_avg_time:.3f} ms\")\n",
    "\n",
    "execution_gpu = benchmark(grade2_numba_no_copy, (a_gpu, b_gpu, a, b, c,), n_repeat=10, n_warmup=2)\n",
    "gpu_avg_time = np.average(execution_gpu.gpu_times) * 1e3  # convert to ms\n",
    "print(f\"Elapsed time grade2_numba_no_copy: {gpu_avg_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e786df-3d99-4227-a583-9a764d46fa92",
   "metadata": {},
   "source": [
    "## Ejercicio C\n",
    "Se ha pasado del código original, que se ejecuta secuencialmente en CPU, a [código asíncrono y concurrente](https://docs.nvidia.com/cuda/cuda-programming-guide/02-basics/asynchronous-execution.html) ejecutado en GPU. Al haber usado la función `benchmark` de *cupy*, se maneja internamente la [sincronización](https://docs.cupy.dev/en/stable/user_guide/performance.html) tanto para cuando usamos *cupy* como con *numba*.\n",
    "\n",
    "En cuanto a los tiempos, podemos ver que, con los datos usados, no hay una gran diferencia entre usar código secuencial optimizado con *numba* o *numpy* y el código ejecutado en GPU cuando se hace copia de datos. Sin embargo, al copiar manualmente los datos a GPU con `cuda.to_device` o generarlos directamente en GPU con funciones de *cupy* como `cp.random.rand`, se obtiene una gran mejora. *Numba* se beneficia más de la copia de datos a GPU. Si vamos a la [documentacion](https://numba.pydata.org/numba-doc/dev/cuda/ufunc.html) de *ufunc* de *numba* podemos leer que *\"The CUDA ufunc adds support for passing intra-device arrays (already on the GPU device) to reduce traffic over the PCI-express bus\"*, que puede ser la razón por la que da mejores tiempos.\n",
    "\n",
    "Existen estudios donde se comparan ambos paquetes como [*Exploring Numba and CuPy for GPU-Accelerated Monte Carlo Radiation Transport*](https://www.researchgate.net/publication/379125485_Exploring_Numba_and_CuPy_for_GPU-Accelerated_Monte_Carlo_Radiation_Transport)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
